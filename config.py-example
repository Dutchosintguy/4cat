""" 4CAT configuration """
import os

# Data source configuration
DATASOURCES = {
	"4chan": {  # should correspond to DATASOURCE in the data source's __init__.py
		"interval": 60,  # scrape interval for boards
		"boards": ["pol", "v"],  # boards to scrape (and generally make available)
		"boards_snapshots": []
	},
	"reddit": {
		"interval": 60,
		"boards": ["europe", "politics"],
		"boards_snapshots": []
	},
	"tumblr": {
		"interval": 60,
		"boards_snapshots": [],
		"expire-datasets": 255600 # Three days - required by the Tumblr API
	}
}

# Configure how the tool is to be named in its web interface. The backend will
# refer to '4CAT' - the name of the software, and a 'powered by 4CAT' notice
# may also show up in the web interface.
TOOL_NAME = "4CAT"
TOOL_NAME_LONG = "4CAT: Capture and Analysis Toolkit"

# Postgres login details
DB_HOST = "localhost"
DB_PORT = 5432
DB_USER = "fourcat"
DB_NAME = "fourcat"
DB_NAME_TEST = "fourcat_test"
DB_PASSWORD = "supers3cr3t"

# Path to folders where logs/images/data may be saved.
# Paths are relative to the folder this config file is in.
PATH_ROOT = os.path.abspath(os.path.dirname(__file__))  # better don't change this
PATH_LOGS = ""  # store logs here - empty means the 4CAT root folder
PATH_IMAGES = "images"  # if left empty or pointing to a non-existent folder, no images will be saved
PATH_DATA = ""  # search results will be stored here as CSV files
PATH_LOCKFILE = "backend"  # the lockfile will be saved in this folder. Probably no need to change!
PATH_SNAPSHOTDATA = ""  # stats for daily snapshots will be stored here

# Path to sphinx data folder. This is used by the script that generates the
# sphinx configuration file for you.
SPHINX_PATH = "/opt/sphinx/data"

# The following two options should be set to ensure that every analysis step can
# be traced to a specific version of 4CAT. This allows for reproducible
# research. You can however leave them empty with no ill effect. The version ID
# should be a commit hash, which will be combined with the Github URL to offer
# links to the exact version of 4CAT code that produced an analysis result
PATH_VERSION = ""  # file containing a version ID (everything after the first whitespace found is ignored)
GITHUB_URL = ""  # URL to the github repository for this 4CAT instance

# 4CAT has an API (available from localhost) that can be used for monitoring
# and will listen for requests on the following port. "0" disables the API.
API_PORT = 4444

# Warning report configuration
WARN_INTERVAL = 600  # every so many seconds, compile a report of logged warnings and send it to admins
WARN_LEVEL = "WARNING"  # only alerts above this level are mailed: DEBUG/INFO/WARNING/ERROR/CRITICAL
WARN_SLACK_URL = ""

# E-mail settings
WARN_EMAILS = []  # e-mail addresses to send warning reports to
ADMIN_EMAILS = []  # e-mail of admins, to send account requests etc to
MAILHOST = "localhost"  # SMTP server to connect to for sending e-mail alerts

# Scrape settings
SCRAPE_TIMEOUT = 5  # how long to wait for a scrape request to finish?
SCRAPE_PROXIES = {"http": []}  # Items in this list should be formatted like "http://111.222.33.44:1234"
IMAGE_INTERVAL = 3600

# YouTube variables to use for processors
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"
YOUTUBE_DEVELOPER_KEY = ""

# Tumblr API keys to use for data capturing
TUMBLR_CONSUMER_KEY = ""
TUMBLR_CONSUMER_SECRET_KEY = ""
TUMBLR_API_KEY = ""
TUMBLR_API_SECRET_KEY = ""

# Web tool settings
class FlaskConfig:
	FLASK_APP = 'webtool/fourcat'
	SECRET_KEY = "REPLACE_THIS"
	SERVER_NAME = '4cat.local:5000'
	SERVER_HTTPS = False  # set to true to make 4CAT use "https" in absolute URLs
	HOSTNAME_WHITELIST = ["localhost"]  # only these may access the web tool; "*" or an empty list matches everything
	HOSTNAME_WHITELIST_API = ["localhost"]  # hostnames matching these are exempt from rate limiting
	HOSTNAME_WHITELIST_NAME = "Automatic login"
